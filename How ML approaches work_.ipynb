{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regressions the following measures are typically used:\n",
    "\n",
    "- Root mean square error (RMSE):\n",
    "\n",
    "$RMSE(X,h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m (h(x^{(i)} - y^{(i)})^2}$\n",
    "\n",
    "- Mean Absolute Error:\n",
    "\n",
    "$MAE(X,h) = \\frac{1}{m} \\sum_{i = 1}^m | h(x^{(i)}) - y^{(i)}|$\n",
    "\n",
    "\n",
    "To discuss the the performance measures for classification problems, we need to remind the notion of confusion matrix. Confusion matrix onsists of true positive, true negative, false positive (wrongly classified as positive), false negative (wrongly classified as negative). \n",
    "\n",
    "Define performance measures for binary classification:\n",
    "\n",
    "- $Precision (sensitivity) = \\frac{TP}{TP + FP}$. Precision is a good measure if only positive outcomes are important for the output. \n",
    "- $Recall = \\frac{TP}{TP + FN}$. \n",
    "- $F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$.  The $F_1$ score is th used if both recall and precision need to be taken into account. \n",
    "- $FP_{rate} = \\frac{FP}{FP + TN}$\n",
    "\n",
    "There is a tradeoff between precision and recall. Let's say that for each instance a model computes a score based on a decision function, and if the score is greater than a threshold, it assigns the instance to the positive class, or else to the negative class. If you  raise the threshold, the false positive becomes a true negative, thereby increasing precision, but one true positive might become a false negative, decreasing recall. In order to account for both, precision and recall, one can select the threshold value that gives the base tradeoff. Another way to select a good tradeoff is plot precision diretly against recall. \n",
    "\n",
    "- The ROC Curve\n",
    "\n",
    "The ROC curves (receiver operating characteristic curve)is another common tool used for binary classifiers. The ROC curve plots the true positive rate (recall) against eh false positive rate. The ROC curve is steeply climbing curve from $0$ to $1$. The curve $x=y$ represents the ROC curve  of a purely random classifier. A good classifier stays as far away from that line as possible. \n",
    "\n",
    "The way to compare classifier is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to $1$, while the  random classifier will have a ROC AUC equal to $0.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance of  a model and their trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's generalization error can be expresses as the sum of three very different errors:\n",
    "\n",
    "- Bias. This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear while it is actually quadratic. A high-bias model is most likely to underfit the training data. \n",
    "- Variance. This part is due to the model's excessive sensitivity to small variations in training data. A model with many degrees of freedom is likely to have high variance and thus to overfit the training data\n",
    "- Irreducible error. This part is due to the noisines of the data. The ony way to reduce this part ofthe error is to clean up the data (remove ouliers, fix the data source, etc)\n",
    "\n",
    "Increasing model's complexity will typicallly increase its variance and reduce its bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assumes that the output is a linear funciton of the input, i.e.\n",
    "\\begin{equation}\n",
    "\\hat{y} =  X \\cdot \\theta + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "The underlying assumptions of the linear regression are:\n",
    "\n",
    "- Linear relationship;\n",
    "\n",
    "- $X$ is of full rank, i.e. there is no clear correltaion between features;\n",
    "\n",
    "- Homoscedascity, i.e. errors have constant variance;\n",
    " \n",
    "- $E[\\epsilon_i| x_j] = 0$ and $\\epsilon_i$ is independent of $x_j$.\n",
    "\n",
    "To estimate the fit of the model, we will use the MSE (Mean Square Error). Recall, that $MSE(X,h) = \\frac{1}{m}  \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2$, where $h(x)$ is the model's evalution at $x$. For the linear regression, there is a closed equation solving thte MSE, namely, $\\hat{\\theta} = (X'X)^{-1}X'y$. \n",
    "\n",
    "Computation complexity: complexity of inverting $n\\times n$ matrix is between $O(n^{2.4})$ to $O(n^3)$. While it is a lot, once it is trained, it is really easy to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are two many features ($n$ is large) , calculating linear regression through the closed form is too expensive. In this case, one can use e.g. stochastic gradient descent or mini-batches. \n",
    "\n",
    "Let us remind how the gradient descent works. If you are given a function $f$, and you want to minimize this function (in our case, cost), then you can start from $y_0$ and then set \n",
    "\\begin{equation}\n",
    "y_k = y_k - f'(y_k) \\times \\gamma,\n",
    "\\end{equation}\n",
    "where $\\gamma$ is the learning rate (pick $\\gamma$ to be between $0$ and $1$). \n",
    "For instance, for the MSE cost function, you get:\n",
    "\\begin{equation}\n",
    "\\beta_k = b_{k-1} - MSE'_{\\beta}(\\beta_{k-1}) \\gamma\n",
    "\\end{equation}\n",
    "\n",
    "NB: features have to be scaled to perform gradient descent, otherwise it'll take a very long time to converge. \n",
    "\n",
    "Then $\\beta_k$ converges to the optimal $\\beta$. The problem with the gradient descent (of large complexity for manu features) still stays. One can use stochastic gradient descent, where only one randomly chosen instance is evaluated at $MSE'$. Something between stochastic gradient descent and batch gradient descent is mini-batch, where at each evalution step $k$ the minibatch of randomly chosen instances is chosen and evaluated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a tool used to reduce overfittting of a model. We will look at three types of regularizations: Ridge, Lasso and Elastic Net (which is a combination of the two). \n",
    "\n",
    "Ridge regression ($l^2$) cost function:\n",
    "\\begin{equation}\n",
    "J(\\theta) = MSE(\\theta) + 0.5 \\alpha \\sum_{i=1}^n \\theta_i^2\n",
    "\\end{equation}\n",
    "\n",
    "This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note, that the regularization terms should only be added to the cost function during training. Note, that larger is $\\alpha$, larger is the penalization of overfitting. \n",
    "\n",
    "Lasso regression ($l^1$) cost function:\n",
    "\\begin{equation}\n",
    "J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^n |\\theta_i|\n",
    "\\end{equation}\n",
    "An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (set them to zero). There is no close form solution for Lasso Regression (for Ridge, there is one); moreover, Lasso cost function is not differentiable at $\\theta_i = 0$. Hence, you finds the solution to the optimization problem by using subgradient ($subg(|x|) = sign(x)$) for Gradient Descent. \n",
    "\n",
    "Elastic Net is a middle ground between Ridge Regression and Lasso regression:\n",
    "\\begin{equation}\n",
    "J(\\theta) = MSE(\\theta) + r\\alpha \\sum_{i=1}^n |\\theta_i| + \\frac{1-r}{2} \\alpha \\sum_{i=1}^n \\theta_i^2\n",
    "\\end{equation}\n",
    "It is advisable to use Elastic Net instead of Lasso Regression due to the Lasso performance (erratic) when the number of features is greater than the number of training instances or when several features are strongly correlated. \n",
    "\n",
    "#### Early Stopping\n",
    "\n",
    "A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. As the epochs go by the algorithm learns, and its prediction error (RMSE) on the training set goes down, along with its prediction error on the validation set. Once the validation error stops decreasing and starts to go back up - this means that the overfit kicks in and it is the time to stop training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model estimated probability is:\n",
    "\\begin{equation}\n",
    "\\hat{p} = h_{\\theta}(x) = \\sigma (\\theta^T \\cdot x),\n",
    "\\end{equation}\n",
    "where $\\sigma$ is a sigmoid function: $\\sigma(t) = \\frac{1}{1 + exp(-t)}$.\n",
    "\n",
    "Then the prediction satisfies:\n",
    "\\begin{equation}\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "0 \\text{ if } \\hat{p} < 0.5\\\\\n",
    "1 \\text{ if } \\hat{p} \\geq 0.5.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Traininng of logistic regression. The way it should work is that it should set the parameter $\\theta$ such that the method estimates high probabilities for positives instances (y = 1) and low probabilities for negative instance (y = 0). The idea is captures by the cost function below for a single training instance:\n",
    "\\begin{equation}\n",
    "c(\\theta) = \n",
    "\\begin{cases}\n",
    "- \\log(\\hat{p}) \\text{ if } y=1\\\\\n",
    "- \\log (1-\\hat{p}) \\text{ if } y = 0.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This cost function makes sense because $-\\log(t)$ grows very large when $t$ approaches $0$ so that cost will be large if the model estimates a probability close to $0$ for a positive instance and it will also be very large if the model estimates a probability close to $1$ for a negative instance. On the other hand, $-\\log(t)$ is close to $0$ when $t$ is close to $1$ so the cost will be close to $0$ if the estimated probability is close to $0$ for a negative instance or close to $1$ for a positive instance, which is precisely what we want. \n",
    "\n",
    "The cost function over the whole training set is simply the average cost over all training. Then the logistic regression cost function will be:\n",
    "\\begin{equation}\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i = 1}^m [y^{(i)} \\log \\hat{p}^{(i)} + (1-y^{(i)} )\\log(1-\\hat{p}^{(i)}]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Generalization to multiple classes through softmax regression. \n",
    "\n",
    "Softmax score for class $k$: $s_k(x) = \\theta_k^T \\cdot x$. Softmax function is then:\n",
    "\\begin{equation}\n",
    "\\hat{p}_k = \\sigma(s(x))_k = \\frac{exp(s_k(x))}{\\sum_{j=1}^K exp(s_j(x))}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of SVM: fitting the widest possible street (street is represented by the solid line as separator and parallel dashed lines as margins) between classes. Adding more training instances \"off the street\" will not affect the decision boundary: it is fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called the support vectors. \n",
    "\n",
    "SVM are sensitive to the feature scales (scale!). If we strictly impose that all instances be off the street and on the right side, this is called hard margin classification. Hard margin classification has two issues: it only works if the data is linearly separable, and it is quite sensitive to outliers. It is preferred therefore to use a more flexible model. \n",
    "\n",
    "The objective is to find a good balance between keeping the street as large as possible and limiting the maring violations (instances that end up in the middle of the streete or even ont he wrong side). This is called soft margin classification. \n",
    "\n",
    "You can control the balance between hard magin and soft margin classification by balancing $C$ hyperparameter. A smaller C value leads to a wider street but more margins violations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM classifier prediction:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} =  \\begin{cases} 0, & \\mbox{if } w^T \\cdot x + b <0 \n",
    "\\\\ 1, & \\mbox{if } w^T \\cdot x + b \\geq 0, \\end{cases}\n",
    "\\end{equation}\n",
    "where $w^T \\cdot x = w_1 x_1 + \\dots + w_n x_n$.\n",
    "\n",
    "The street (dashed line) are generated by $h = w^T \\cdot x + b = \\pm 1$, while the center of the street (solid line) is generated by $h=0$.\n",
    "\n",
    "Hard margin linear SVM classifier objective:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{minimize}_{w,b} \\text{ }  0.5 w^T \\cdot w\\\\\n",
    "&\\text{subject to: } t^{(i)} (w^T \\cdot x^{(i)} + b) \\geq 1, \\text{ for } i = 1, \\dots, n\n",
    "\\end{align*}\n",
    "\n",
    "Soft margin linear SVM classifier objective:\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{minimize}_{w,b,\\xi} \\text{ }  0.5 w^T \\cdot w + C \\sum_{i=1}^m \\xi^{(i)}\\\\\n",
    "&\\text{subject to: } t^{(i)} (w^T \\cdot x^{(i)} + b) \\geq 1 - \\xi^{(i)}, \\text{ for } \\xi^{(i)}\\geq 0 \\text{ and } i = 1, \\dots, n\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are ofet called white box models as they are easy to interpret. Once the deciision is built, it is very easy to make predictions. For any instance, you make a decision and descent along the leaf. The decision is trained on a trianing set. At each node you get a \"condition\" with respect to each you make a decision. Moreover, you have a gini calculated for each node. Gini impurity is defined as $G_i = 1 - \\sum_{k = 1}^m p_{i,k}^2$, where $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i$-th node. The node is pure if gini = 0 (that is, if all instances belong to the \"correct\" class). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note, that Scikit-Learn uses the Classification and regression tree (CART) algorithm, which produces only binary trees: nonleaf nodes always have two children. The CART works is that it splits the training set in two subsets using a single feature $k$ and a threshold $t_k$. How does it choose $k$ and $t_k$? It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size). More precisely, CART minimizes the following cost function:\n",
    "\\begin{equation}\n",
    "J(k, t_k) = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right},\n",
    "\\end{equation}\n",
    "where $G_{left/right}$ measures the impurity of the left/right subset and $m_{left/right}$ is the number of instances in the left/right subset. \n",
    "\n",
    "Once CART has successfully split the training set in two, it splits the subsets using the same logic. It stops recursing once it reaches the maximum depth or if it cannot find a split that will recue impurity. \n",
    "\n",
    "Complexity. Making prediction requires going through roughtly $O(log_2(m))$ nodes. Since each node only requires checking the value of one feature, the overall prediction complexity is just $O(log_2(m))$. However, the training algorithm compares all features on all samples at each node. This results in a training complexity of $O(n\\times m log(m))$, where $n$ is the number of features. \n",
    "\n",
    "Choice of impurity measure. We have discussed Gini impurity above, however there is another measure called entropy: $H_i = - \\sum_{k=1, p_{i,k}\\neq 0}^{n} p_{i,k} log(p_{i,k})$. \n",
    "\n",
    "Decision Trees make very few assumptions about the training data. If left unconstained, the tree structure will adapt itself to the training data, fitting it very closely and most likely overfitting it. Regularization can be done by, for instance, limiting the number of maximum depth. \n",
    "\n",
    "\n",
    "Decision Trees are used typically for classification, but can be used equally for regression. The way it works is that once the instances are split into \"classes\", the average of the output is taken. The optimization (minimization) is done one the MSE function. \n",
    "\n",
    "Decision Trees are know to perform well when the separation is possible vertically and horizontally. However, if not, they don't perform that well (e.g. classes are linearly separable, but the line is at 45 degrees). One of the ways to deal with this is to rotate (use PCA, for instance). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Learning and Random Forests\n",
    "\n",
    "If one aggregates the predictions of a group of predictors, you will often get better predictions that with the best individual predictor. A group of predictirs is called an ensemble; thus, this technique is called Ensemble Learning. \n",
    "\n",
    "- Hard voting. To create an aggregated classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier. \n",
    "\n",
    "- If all classifiers are able to estimate class probabilities, then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called soft voting. This performs even better than the hard voting. \n",
    "\n",
    "Note, that even all classifiers are weak learners, the ensemble can still be a strong learner (law of large numbers). The best way to improve the performance of ensemble, is to have a very diverse set of learners. Another way is to use the same training algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is performed with replacemtn, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it is called pasting. \n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the prediction of all predictors. The aggregation function is typically the statistical mode (he mode of a set of data is the one that occurs most) or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set. \n",
    "\n",
    "Random Forest is an ensemble of Decision Trees, generally trained via the bagging method. Typically with max samples set to the size of the traiing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting refers to Ensemble method that can combine several weak learners into a strong leraner. \n",
    "Idea: train predictors sequentially, each trying to correct its predecessor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADABoost\n",
    "\n",
    "Algorithm first trains a base classifier (e.g. Decision Tree) and uses it to make predictions on the training set:\n",
    "1. Choose classifier, make preidiction on training set\n",
    "2. Misclassified training instances get higher weights. Train the classifier using updated weights and make predictions\n",
    "3. Repeat Step 2. \n",
    "THe resulting predictors are ensembled using bagging with weights depending on their overall accuracy on training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosting\n",
    "\n",
    "As opposed to ADABoost, Gradient boosting sequentially adds predictors to fit them to residual errors made by the previous predictor. \n",
    "1. Fit e.g. Decition Tree Regressor method to training set\n",
    "2. Train second Decition Tree on the residual errors made by the first predictors\n",
    "3. Train a third regressor on the residual errors made by the second predictor. \n",
    "Final predictor is the sum of the three above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a conditional probabiltiy model: given a problme instance to be classified $x = (x_1, \\dots, x_n)$ with some $n$ features (independent), it assigns to this instance probabilities $p(C_k|x_1, \\dots, x_n)$ for each of $K$ possible classes $C_k$. Or, in other words:\n",
    "\\begin{align}\n",
    "p(C_k|x) & = \\frac{p(C_k) p(x| C_k)}{p(x)}\\\\\n",
    "posterior &= \\frac{prior \\times likelihood}{evidence}\n",
    "\\end{align}\n",
    "\n",
    "Note, that denominator does not depend on $C$ and is a constant. The numerator should be $p(C_k, x)$, but it is rewritten as $p(x_1|x_2, \\dots, x_n, C_k) p(x_2, \\dots, x_n, C_k) = \\dots p(x_1|x_2, \\dots, x_n, C_k)\\dots p(x_n|C_k)p(C_k)$. But, we assumed that features are independent, hence, the above is simply\n",
    "$p(x_1| C_k)\\dots p(x_n|C_k)p(C_k)$.\n",
    "\n",
    "\n",
    "The classifier from this model is constructed as:\n",
    "\\begin{equation}\n",
    "\\hat{y} = argmax_{k \\in \\{1, \\dots, K\\}} p(C_k) \\prod_{i=1}^n p(x_i| C_k)\n",
    "\\end{equation}\n",
    "In other words, we maximize the posterior probability and it is called a MAP decision rule. \n",
    "\n",
    "A class's prior may be calculated by assumingequiprobable classess or by calculating an estimate for the class probability from the training set. To estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the feature (Gaussian, Multinomial, etc.\n",
    "\n",
    "Disadvantage: independence of features does not hold usually, but it does not prevent Naive Bayes to work quite well in many applications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The perceptron\n",
    "\n",
    "The Perceptron is one of the simples ANN (Artiifical Neural Networks). It is based on a slightly different artifical neuron called a linear threshold unit (LTU): $h_w(x) = step(z) = step (w^T \\cdot x)$, where $w^T \\cdot x = w_0 + \\sum_{i = 1}^n w_i x_i$. Note, that the step function can help easily in a classification problem. \n",
    "Common step functions used in Perceptrons are heaviside ($0$ if $x<0$ and $1$ otherwise) or sign(x)  which takes three values. \n",
    "\n",
    "Perceptron learning rule works as follows:\n",
    "\\begin{equation}\n",
    "w_{i,j}^{k+1} = w_{i,j}^k + \\eta (\\hat{y}_j - y_j) x_j, \n",
    "\\end{equation}\n",
    "where\n",
    "- $w_{i,j}$ is the connection weight between the i-th input neuron and the j-th output neuron\n",
    "- $x_i$ is the i-th input value of the current training instance\n",
    "- $\\hat{y}_j$ is the output of the $j$-th output neuron for the current training instance\n",
    "- $y_i$ is the target output of the j-th output neuron for the current training instance\n",
    "- $\\eta$ is the learning rate\n",
    "The decision boundary of each output neuron is linear, so Perceptrons are incapable of leaning complex patterns (like logistic regression). If the instances are linearly separable then Rosenblatt deomstrated that this algo would converge to a solution (perceptron convergence theorem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning: compact description\n",
    "\n",
    "The way it works is the following. Let $W^{(l)}$ denote the weight matrix that connect layer $l-1$ to layer $l$. The matrix $W^{(1)}$ is of dimension $D\\times K$, matrices $W^{(l)}$ for $2\\leq l < L$ are of dimensions $K\\times K$ and the matrix $W^{(l+1)}$  is of dimension $K\\times 1$. The entries of each matrix are given as $W_{i,j}^{l} = w_{i,j}^{(l)}$. We also have the bias vector $b^{l}$ for $1\\leq i \\leq L+1$ that collects the bias term. All these vectors are of length $K$ except for the last one which is a scalar. \n",
    "\n",
    "For each layer then you get $x^l = f^l (x^{l-1}) = \\phi((W^l)^T x^{l-1} + b^l)$. Then the overall function $y = f(x_0)$ can be written as $f(x_0) = f^{l+1} \\cdot \\dots \\cdot f^1 (x_0)$.\n",
    "\n",
    "The cost function is defined as:\n",
    "\\begin{equation}\n",
    "L = \\frac{1}{N} \\sum_{n=1}^N (y_n - f^{L+1}\\cdot \\dots \\cdot f^1 (x_n^0))^2\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals of clustering is to group similar instances together into clusters. Just like classification, each instance gets assigned to a group; unlike classification, clustering is an unsupervised task. NOte, that if a dataset is labeled, you can you alogrithms such as Logistic Regression, SVM, or Random Forest classifiers. \n",
    "\n",
    "K-means clustering can be used for customer segmentation, data analysis (to analyze a new data set), outlier detection.\n",
    "\n",
    "K-means algorithm:\n",
    "- Start with placing the centroids of clusters randomly (yes, yu will have to decide the number of clusters in the beginning)\n",
    "- Label the instances (wrt to centroid which is the closest)\n",
    "- Update the centroids (as a mean of instances corresponding to the respective cluster)\n",
    "- Etc. until centroids stop moving. \n",
    "\n",
    "Although the algorithm is guaranteed to converge, it may not converge to the righ solution (it may converge to a local optimum). To mitigate this one may choose the initialization method. \n",
    "- Sometimes you can guess the centers (by plotting the data, it might become obvious). \n",
    "- If not, then the most naive method would be to set the centers randomly (uniformly) and perform K-means a few times to confirm the best solution. A performance metric in this case can be model's interia, which is the mean squared distance between each instance and its closest centroid. Better models have lower intertia. \n",
    "- A significant improvement to the K-Means initialization was done in 2006: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
